{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal Innovator:\n",
    "- Retrocausality project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install mesa\n",
    "#pip install ipykernel\n",
    "#pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mesa import Agent, Model\n",
    "from mesa.space import MultiGrid\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1: [(6, 0), (5, 0), (5, 1), (5, 2), (5, 3), (4, 3)]\n",
      "Agent 2: [(7, 4), (7, 5), (6, 5), (7, 5), (7, 4), (6, 4)]\n",
      "Agent 3: [(6, 8), (6, 9), (7, 9), (6, 9), (6, 8), (6, 7)]\n",
      "Agent 4: [(7, 6), (8, 6), (7, 6), (6, 6), (7, 6), (7, 7)]\n",
      "Agent 5: [(5, 1), (4, 1), (4, 0), (5, 0), (5, 1), (6, 1)]\n"
     ]
    }
   ],
   "source": [
    "class TimeAgent(Agent):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(model) # Mesa 3: only model is passed\n",
    "        self.model = model  # Explicitly store model reference if needed\n",
    "        self.positions = [] # Will be populated with initial position in TimeModel\n",
    "\n",
    "    def step(self):\n",
    "        attempts = 0\n",
    "        max_attempts = 10  # Prevent infinite loops\n",
    "        while attempts < max_attempts:\n",
    "            x, y = self.pos\n",
    "            move = random.choice([(0, 1), (0, -1), (1, 0), (-1, 0)]) # Move randomly: up, down, left, right\n",
    "            new_pos = (x + move[0], y + move[1])\n",
    "            if (0 <= new_pos[0] < self.model.grid.width) and (0 <= new_pos[1] < self.model.grid.height):\n",
    "                if new_pos not in self.model.occupied_positions or new_pos == self.pos:\n",
    "                    self.model.grid.move_agent(self, new_pos)\n",
    "                    self.model.occupied_positions.discard(self.pos) # Remove old position\n",
    "                    self.model.occupied_positions.add(new_pos) # Add new position\n",
    "                    self.positions.append(new_pos) # Record new position\n",
    "                    break\n",
    "            attempts += 1\n",
    "        # If no valid move is found after max_attempts, agent stays put (position unchanged)\n",
    "        else:\n",
    "            self.positions.append(self.pos)  # Explicitly stay put\n",
    "\n",
    "          \n",
    "class TimeModel(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.grid = MultiGrid(10, 10, False)  # 10x10 grid, torus disabled\n",
    "        self.schedule = []  # Manual agent list\n",
    "        self.random = random.Random()\n",
    "        self.step_count = 0  # Track step number\n",
    "        self.occupied_positions = set()  # Track occupied positions\n",
    "        \n",
    "        # Create 5 agents with unique random starting positions\n",
    "        available_positions = [(x, y) for x in range(10) for y in range(10)]  # All 10x10 positions\n",
    "        self.random.shuffle(available_positions)  # Randomize order\n",
    "        for i in range(5):\n",
    "            agent = TimeAgent(self)\n",
    "            start_pos = available_positions[i]  # Take a unique position\n",
    "            self.grid.place_agent(agent, start_pos)\n",
    "            agent.pos = start_pos  # Explicitly set pos (Mesa 3 compatibility)\n",
    "            \n",
    "            agent.positions.append(start_pos)  # Record initial position\n",
    "            self.occupied_positions.add(start_pos)\n",
    "            self.schedule.append(agent)    \n",
    "\n",
    "    def step(self):\n",
    "        # Reset occupied positions for this step (will be rebuilt)\n",
    "        self.occupied_positions.clear()\n",
    "        for agent in self.schedule:\n",
    "            self.occupied_positions.add(agent.pos)\n",
    "        \n",
    "        random.shuffle(self.schedule)  # Random activation\n",
    "        for agent in self.schedule:\n",
    "            agent.step()\n",
    "        self.step_count += 1  # Increment before printing    \n",
    "        # self.print_positions()\n",
    "        \n",
    "\n",
    "    # Return the list of position histories for all agents\n",
    "    def get_positions(self):\n",
    "        sorted_agents = sorted(self.schedule, key=lambda a: a.unique_id)\n",
    "        return [agent.positions for agent in sorted_agents]\n",
    "    \n",
    "    # Print history positions of all agents\n",
    "    def print_positions(self):\n",
    "        sorted_agents = sorted(self.schedule, key=lambda a: a.unique_id)\n",
    "        for agent in sorted_agents:\n",
    "            print(f\"Agent {agent.unique_id}: {agent.positions}\")\n",
    "\n",
    "\n",
    "model = TimeModel()\n",
    "\n",
    "for _ in range(5):\n",
    "    model.step()\n",
    "model.print_positions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100/2000 runs\n",
      "Completed 200/2000 runs\n",
      "Completed 300/2000 runs\n",
      "Completed 400/2000 runs\n",
      "Completed 500/2000 runs\n",
      "Completed 600/2000 runs\n",
      "Completed 700/2000 runs\n",
      "Completed 800/2000 runs\n",
      "Completed 900/2000 runs\n",
      "Completed 1000/2000 runs\n",
      "Completed 1100/2000 runs\n",
      "Completed 1200/2000 runs\n",
      "Completed 1300/2000 runs\n",
      "Completed 1400/2000 runs\n",
      "Completed 1500/2000 runs\n",
      "Completed 1600/2000 runs\n",
      "Completed 1700/2000 runs\n",
      "Completed 1800/2000 runs\n",
      "Completed 1900/2000 runs\n",
      "Completed 2000/2000 runs\n",
      "Saved 2000 runs to abm_data_with_rule.pkl\n",
      "Loaded 2000 runs with rule\n",
      "Training data shape: X=torch.Size([300000, 5, 12]), y=torch.Size([300000])\n",
      "\n",
      "Training TCN (Direction Prediction with Rule)...\n",
      "Epoch [10/300], Train Loss: 3.7390, Val Loss: 3.6636, Train Acc: 0.2126, Val Acc: 0.2430, LR: 0.001000\n",
      "Epoch [20/300], Train Loss: 3.7366, Val Loss: 3.6650, Train Acc: 0.2127, Val Acc: 0.2430, LR: 0.001000\n",
      "Epoch [30/300], Train Loss: 3.7356, Val Loss: 3.6635, Train Acc: 0.2131, Val Acc: 0.2432, LR: 0.001000\n",
      "Epoch [40/300], Train Loss: 3.7356, Val Loss: 3.6645, Train Acc: 0.2129, Val Acc: 0.2433, LR: 0.001000\n",
      "Epoch [50/300], Train Loss: 3.7364, Val Loss: 3.6649, Train Acc: 0.2128, Val Acc: 0.2432, LR: 0.000500\n",
      "Epoch [60/300], Train Loss: 3.2969, Val Loss: 3.1434, Train Acc: 0.2661, Val Acc: 0.2792, LR: 0.000500\n",
      "Epoch [70/300], Train Loss: 3.2973, Val Loss: 3.1394, Train Acc: 0.2661, Val Acc: 0.2838, LR: 0.000500\n",
      "Epoch [80/300], Train Loss: 3.2985, Val Loss: 3.1379, Train Acc: 0.2657, Val Acc: 0.2836, LR: 0.000500\n",
      "Epoch [90/300], Train Loss: 3.2948, Val Loss: 3.1354, Train Acc: 0.2669, Val Acc: 0.2854, LR: 0.000500\n",
      "Epoch [100/300], Train Loss: 3.2962, Val Loss: 3.1360, Train Acc: 0.2668, Val Acc: 0.2887, LR: 0.000250\n",
      "Early stopping triggered after 105 epochs\n",
      "Model saved to tcn_model.pth\n",
      "\n",
      "No Rule Move Frequencies: {'Up (0, 1)': 0.24271551155115512, 'Down (0, -1)': 0.24182508250825083, 'Right (1, 0)': 0.24637689768976898, 'Left (-1, 0)': 0.24606204620462047, 'No move (0, 0)': 0.023020462046204622}\n",
      "With Rule Move Frequencies: {'Up (0, 1)': 0.20876831683168318, 'Down (0, -1)': 0.21133531353135313, 'Right (1, 0)': 0.15998085808580859, 'Left (-1, 0)': 0.16286072607260726, 'No move (0, 0)': 0.2570547854785479}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import random\n",
    "from mesa import Agent, Model\n",
    "from mesa.space import MultiGrid\n",
    "\n",
    "# TimeAgent with New Rule\n",
    "class TimeAgent(Agent):\n",
    "    def __init__(self, model, use_rule=True):\n",
    "        super().__init__(model)\n",
    "        self.model = model\n",
    "        self.positions = []\n",
    "        self.use_rule = use_rule\n",
    "\n",
    "    def step(self):\n",
    "        global trained_model\n",
    "        x, y = self.pos\n",
    "        moves = [(0, 1), (-1, 0), (0, -1), (1, 0)]  # Up, Left, Down, Right\n",
    "        \n",
    "        avoid_move = None\n",
    "        if len(self.positions) >= 5 and trained_model is not None and self.use_rule:\n",
    "            seq = self.positions[-5:]\n",
    "            others = [a.pos for a in self.model.schedule if a != self]\n",
    "            # Prepare input: 5 steps, each with (x, y) + 10 features\n",
    "            input_data = []\n",
    "            for pos in seq:\n",
    "                step_features = list(pos)  # [x, y]\n",
    "                # Add 5 other agents' relative positions (10 values)\n",
    "                for i in range(min(5, len(others))):\n",
    "                    ox, oy = others[i]\n",
    "                    step_features.extend([ox - pos[0], oy - pos[1]])\n",
    "                while len(step_features) < 12:  # Pad to 12\n",
    "                    step_features.extend([0, 0])\n",
    "                input_data.append(step_features[:12])\n",
    "            input_data = torch.tensor([input_data], dtype=torch.float32) / 9.0\n",
    "            with torch.no_grad():\n",
    "                pred_dir = trained_model(input_data).argmax().item()\n",
    "            avoid_move = list(reverse_map.keys())[pred_dir]\n",
    "\n",
    "        attempts = 0\n",
    "        max_attempts = 10\n",
    "        move_idx = random.randint(0, 3)\n",
    "\n",
    "        if self.use_rule:\n",
    "            while attempts < max_attempts:\n",
    "                move = moves[move_idx]\n",
    "                if move == avoid_move and attempts < max_attempts - 1:\n",
    "                    move_idx = (move_idx + 1) % 4\n",
    "                    attempts += 1\n",
    "                    continue\n",
    "                new_pos = (x + move[0], y + move[1])\n",
    "                if (0 <= new_pos[0] < self.model.grid.width) and (0 <= new_pos[1] < self.model.grid.height):\n",
    "                    if new_pos not in self.model.occupied_positions or new_pos == self.pos:\n",
    "                        self.model.grid.move_agent(self, new_pos)\n",
    "                        self.model.occupied_positions.discard(self.pos)\n",
    "                        self.model.occupied_positions.add(new_pos)\n",
    "                        self.positions.append(new_pos)\n",
    "                        break\n",
    "                left_idx = (move_idx + 1) % 4\n",
    "                left_pos = (x + moves[left_idx][0], y + moves[left_idx][1])\n",
    "                if (0 <= left_pos[0] < self.model.grid.width) and (0 <= left_pos[1] < self.model.grid.height) and left_pos not in self.model.occupied_positions:\n",
    "                    self.model.grid.move_agent(self, left_pos)\n",
    "                    self.model.occupied_positions.discard(self.pos)\n",
    "                    self.model.occupied_positions.add(left_pos)\n",
    "                    self.positions.append(left_pos)\n",
    "                    break\n",
    "                opp_idx = (move_idx + 2) % 4\n",
    "                opp_pos = (x + moves[opp_idx][0], y + moves[opp_idx][1])\n",
    "                if (0 <= opp_pos[0] < self.model.grid.width) and (0 <= opp_pos[1] < self.model.grid.height) and opp_pos not in self.model.occupied_positions:\n",
    "                    self.model.grid.move_agent(self, opp_pos)\n",
    "                    self.model.occupied_positions.discard(self.pos)\n",
    "                    self.model.occupied_positions.add(opp_pos)\n",
    "                    self.positions.append(opp_pos)\n",
    "                    break\n",
    "                self.positions.append(self.pos)\n",
    "                break\n",
    "        else:\n",
    "            while attempts < max_attempts:\n",
    "                move = random.choice(moves)\n",
    "                new_pos = (x + move[0], y + move[1])\n",
    "                if (0 <= new_pos[0] < self.model.grid.width) and (0 <= new_pos[1] < self.model.grid.height):\n",
    "                    if new_pos not in self.model.occupied_positions or new_pos == self.pos:\n",
    "                        self.model.grid.move_agent(self, new_pos)\n",
    "                        self.model.occupied_positions.discard(self.pos)\n",
    "                        self.model.occupied_positions.add(new_pos)\n",
    "                        self.positions.append(new_pos)\n",
    "                        break\n",
    "                attempts += 1\n",
    "            else:\n",
    "                self.positions.append(self.pos)\n",
    "\n",
    "# TimeModel\n",
    "class TimeModel(Model):\n",
    "    def __init__(self, use_rule=True):\n",
    "        super().__init__()\n",
    "        self.grid = MultiGrid(10, 10, False)\n",
    "        self.schedule = []\n",
    "        self.random = random.Random()\n",
    "        self.step_count = 0\n",
    "        self.occupied_positions = set()\n",
    "        self.use_rule = use_rule\n",
    "        \n",
    "        available_positions = [(x, y) for x in range(10) for y in range(10)]\n",
    "        self.random.shuffle(available_positions)\n",
    "        for i in range(30):\n",
    "            agent = TimeAgent(self, use_rule=self.use_rule)\n",
    "            start_pos = available_positions[i]\n",
    "            self.grid.place_agent(agent, start_pos)\n",
    "            agent.pos = start_pos\n",
    "            agent.positions.append(start_pos)\n",
    "            self.occupied_positions.add(start_pos)\n",
    "            self.schedule.append(agent)\n",
    "\n",
    "    def step(self):\n",
    "        self.occupied_positions.clear()\n",
    "        for agent in self.schedule:\n",
    "            self.occupied_positions.add(agent.pos)\n",
    "        random.shuffle(self.schedule)\n",
    "        for agent in self.schedule:\n",
    "            agent.step()\n",
    "        self.step_count += 1\n",
    "\n",
    "    def get_positions(self):\n",
    "        sorted_agents = sorted(self.schedule, key=lambda a: a.unique_id)\n",
    "        return [(agent.positions, [a.pos for a in sorted_agents if a != agent]) for agent in sorted_agents]\n",
    "\n",
    "# TCN\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size=12, output_size=5, num_channels=[64, 64, 64], kernel_size=5, dropout=0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(num_channels)):\n",
    "            dilation = 2 ** i\n",
    "            in_channels = input_size if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            layers.append(nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            if padding > 0:\n",
    "                layers.append(nn.ConstantPad1d((-padding, 0), 0))\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # [batch_size, input_size, seq_len]\n",
    "        out = self.tcn(x)\n",
    "        out = out[:, :, -1]  # Last time step\n",
    "        return out\n",
    "\n",
    "# Data collection\n",
    "def collect_and_save_data(num_runs=2000, filename=\"abm_data_with_rule.pkl\", use_rule=True):\n",
    "    all_data = []\n",
    "    for run in range(num_runs):\n",
    "        model = TimeModel(use_rule=use_rule)\n",
    "        positions_history = []\n",
    "        for _ in range(10):\n",
    "            model.step()\n",
    "            positions_history.append(model.get_positions())\n",
    "        all_data.append(positions_history)\n",
    "        if (run + 1) % 100 == 0:\n",
    "            print(f\"Completed {run + 1}/{num_runs} runs\")\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(all_data, f)\n",
    "    print(f\"Saved {len(all_data)} runs to {filename}\")\n",
    "    return all_data\n",
    "\n",
    "# Prepare data (Fixed)\n",
    "def prepare_training_data(data, seq_len=5):\n",
    "    X, y = [], []\n",
    "    direction_map = {(0, 1): 0, (0, -1): 1, (1, 0): 2, (-1, 0): 3, (0, 0): 4}\n",
    "    for run_data in data:\n",
    "        for step_idx in range(len(run_data) - seq_len):\n",
    "            step_data = run_data[step_idx:step_idx + seq_len]\n",
    "            for positions, others in step_data[-1]:\n",
    "                if len(positions) < seq_len + 1:\n",
    "                    continue\n",
    "                seq = positions[-seq_len - 1:-1]  # 5 steps\n",
    "                seq_data = []\n",
    "                for pos in seq:\n",
    "                    step_features = list(pos)  # [x, y]\n",
    "                    # Add 5 other agents' relative positions (10 values)\n",
    "                    for i in range(min(5, len(others))):\n",
    "                        ox, oy = others[i]\n",
    "                        step_features.extend([ox - pos[0], oy - pos[1]])\n",
    "                    while len(step_features) < 12:  # Pad to 12\n",
    "                        step_features.extend([0, 0])\n",
    "                    seq_data.append(step_features[:12])\n",
    "                X.append(seq_data)\n",
    "                # Target\n",
    "                x1, y1 = positions[-2]\n",
    "                x2, y2 = positions[-1]\n",
    "                direction = (x2 - x1, y2 - y1)\n",
    "                y.append(direction_map[direction])\n",
    "    X = torch.tensor(X, dtype=torch.float32) / 9.0\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    return X, y\n",
    "\n",
    "# Training\n",
    "def train_tcn(X, y, epochs=300, batch_size=32, learning_rate=0.001, patience=20):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = TCN(input_size=12, output_size=5, num_channels=[64, 64, 64], kernel_size=5, dropout=0.2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == batch_y).sum().item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                val_correct += (outputs.argmax(dim=1) == batch_y).sum().item()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        train_acc = train_correct / len(X_train)\n",
    "        val_acc = val_correct / len(X_val)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss_avg:.4f}, Val Loss: {val_loss_avg:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    torch.save(model.state_dict(), \"tcn_model.pth\")\n",
    "    print(\"Model saved to tcn_model.pth\")\n",
    "    return model\n",
    "\n",
    "# Global variables\n",
    "trained_model = None\n",
    "direction_map = {0: \"Up (0, 1)\", 1: \"Down (0, -1)\", 2: \"Right (1, 0)\", 3: \"Left (-1, 0)\", 4: \"No move (0, 0)\"}\n",
    "reverse_map = {(0, 1): 0, (0, -1): 1, (1, 0): 2, (-1, 0): 3, (0, 0): 4}\n",
    "\n",
    "# Train with rule\n",
    "sequences_with_rule = collect_and_save_data(num_runs=2000, filename=\"abm_data_with_rule.pkl\", use_rule=True)\n",
    "X, y = prepare_training_data(sequences_with_rule)\n",
    "print(f\"Loaded {len(sequences_with_rule)} runs with rule\")\n",
    "print(f\"Training data shape: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "print(\"\\nTraining TCN (Direction Prediction with Rule)...\")\n",
    "trained_model = train_tcn(X, y, epochs=300, batch_size=32, learning_rate=0.001, patience=20)\n",
    "\n",
    "# Compare with vs. without rule\n",
    "def compare_rule_effects(runs=100, steps=10):\n",
    "    no_rule_model = TimeModel(use_rule=False)\n",
    "    no_rule_moves = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    for _ in range(runs):\n",
    "        for _ in range(steps):\n",
    "            no_rule_model.step()\n",
    "        for agent in no_rule_model.schedule:\n",
    "            for i in range(len(agent.positions) - 1):\n",
    "                dx, dy = agent.positions[i + 1][0] - agent.positions[i][0], agent.positions[i + 1][1] - agent.positions[i][1]\n",
    "                move_idx = reverse_map.get((dx, dy), 4)\n",
    "                no_rule_moves[move_idx] += 1\n",
    "\n",
    "    with_rule_model = TimeModel(use_rule=True)\n",
    "    with_rule_moves = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    for _ in range(runs):\n",
    "        for _ in range(steps):\n",
    "            with_rule_model.step()\n",
    "        for agent in with_rule_model.schedule:\n",
    "            for i in range(len(agent.positions) - 1):\n",
    "                dx, dy = agent.positions[i + 1][0] - agent.positions[i][0], agent.positions[i + 1][1] - agent.positions[i][1]\n",
    "                move_idx = reverse_map.get((dx, dy), 4)\n",
    "                with_rule_moves[move_idx] += 1\n",
    "\n",
    "    total_no_rule = sum(no_rule_moves.values())\n",
    "    total_with_rule = sum(with_rule_moves.values())\n",
    "    print(\"\\nNo Rule Move Frequencies:\", {direction_map[k]: v/total_no_rule for k, v in no_rule_moves.items()})\n",
    "    print(\"With Rule Move Frequencies:\", {direction_map[k]: v/total_with_rule for k, v in with_rule_moves.items()})\n",
    "\n",
    "# Run comparison\n",
    "compare_rule_effects(runs=100, steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100/2000 runs\n",
      "Completed 200/2000 runs\n",
      "Completed 300/2000 runs\n",
      "Completed 400/2000 runs\n",
      "Completed 500/2000 runs\n",
      "Completed 600/2000 runs\n",
      "Completed 700/2000 runs\n",
      "Completed 800/2000 runs\n",
      "Completed 900/2000 runs\n",
      "Completed 1000/2000 runs\n",
      "Completed 1100/2000 runs\n",
      "Completed 1200/2000 runs\n",
      "Completed 1300/2000 runs\n",
      "Completed 1400/2000 runs\n",
      "Completed 1500/2000 runs\n",
      "Completed 1600/2000 runs\n",
      "Completed 1700/2000 runs\n",
      "Completed 1800/2000 runs\n",
      "Completed 1900/2000 runs\n",
      "Completed 2000/2000 runs\n",
      "Saved 2000 runs to abm_data_with_rule_2.pkl\n",
      "Loaded 2000 runs with rule\n",
      "Training data shape: X=torch.Size([300000, 5, 60]), y=torch.Size([300000])\n",
      "\n",
      "Training TCN (Direction Prediction with Rule)...\n",
      "Epoch [10/300], Train Loss: 2.3149, Val Loss: 1.7476, Train Acc: 0.3431, Val Acc: 0.3700, LR: 0.001000\n",
      "Epoch [20/300], Train Loss: 2.2849, Val Loss: 1.7241, Train Acc: 0.3629, Val Acc: 0.3876, LR: 0.001000\n",
      "Epoch [30/300], Train Loss: 2.2738, Val Loss: 1.7110, Train Acc: 0.3703, Val Acc: 0.3972, LR: 0.001000\n",
      "Epoch [40/300], Train Loss: 2.2565, Val Loss: 1.6947, Train Acc: 0.3792, Val Acc: 0.4041, LR: 0.001000\n",
      "Early stopping triggered after 48 epochs\n",
      "Model saved to tcn_model_2.pth\n",
      "\n",
      "No Rule Move Frequencies: {'Up (0, 1)': 0.24264488448844884, 'Down (0, -1)': 0.24312475247524754, 'Right (1, 0)': 0.24544620462046204, 'Left (-1, 0)': 0.24414587458745873, 'No move (0, 0)': 0.02463828382838284}\n",
      "With Rule Move Frequencies: {'Up (0, 1)': 0.23725808580858085, 'Down (0, -1)': 0.23763168316831684, 'Right (1, 0)': 0.23194521452145214, 'Left (-1, 0)': 0.23092145214521453, 'No move (0, 0)': 0.062243564356435646}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import random\n",
    "from mesa import Agent, Model\n",
    "from mesa.space import MultiGrid\n",
    "\n",
    "# TimeAgent with Simpler Rule\n",
    "class TimeAgent(Agent):\n",
    "    def __init__(self, model, use_rule=True):\n",
    "        super().__init__(model)\n",
    "        self.model = model\n",
    "        self.positions = []\n",
    "        self.use_rule = use_rule\n",
    "\n",
    "    def step(self):\n",
    "        global trained_model\n",
    "        x, y = self.pos\n",
    "        moves = [(0, 1), (-1, 0), (0, -1), (1, 0)]\n",
    "        \n",
    "        avoid_move = None\n",
    "        if len(self.positions) >= 5 and trained_model is not None and self.use_rule:\n",
    "            seq = self.positions[-5:]\n",
    "            others = [a.pos for a in self.model.schedule if a != self]\n",
    "            input_data = []\n",
    "            for pos in seq:\n",
    "                step_features = list(pos)\n",
    "                for ox, oy in others:  # All 29\n",
    "                    step_features.extend([ox - pos[0], oy - pos[1]])\n",
    "                while len(step_features) < 60:\n",
    "                    step_features.extend([0, 0])\n",
    "                input_data.append(step_features[:60])\n",
    "            input_data = torch.tensor([input_data], dtype=torch.float32) / 9.0\n",
    "            with torch.no_grad():\n",
    "                pred_dir = trained_model(input_data).argmax().item()\n",
    "            avoid_move = list(reverse_map.keys())[pred_dir]\n",
    "\n",
    "        attempts = 0\n",
    "        max_attempts = 10\n",
    "        move_idx = random.randint(0, 3)\n",
    "\n",
    "        if self.use_rule:\n",
    "            while attempts < max_attempts:\n",
    "                move = moves[move_idx]\n",
    "                if move == avoid_move and attempts < max_attempts - 1:\n",
    "                    move_idx = (move_idx + 1) % 4\n",
    "                    attempts += 1\n",
    "                    continue\n",
    "                new_pos = (x + move[0], y + move[1])\n",
    "                if (0 <= new_pos[0] < self.model.grid.width) and (0 <= new_pos[1] < self.model.grid.height):\n",
    "                    if new_pos not in self.model.occupied_positions or new_pos == self.pos:\n",
    "                        self.model.grid.move_agent(self, new_pos)\n",
    "                        self.model.occupied_positions.discard(self.pos)\n",
    "                        self.model.occupied_positions.add(new_pos)\n",
    "                        self.positions.append(new_pos)\n",
    "                        break\n",
    "                move_idx = (move_idx + 1) % 4\n",
    "                attempts += 1\n",
    "            else:\n",
    "                self.positions.append(self.pos)\n",
    "        else:\n",
    "            while attempts < max_attempts:\n",
    "                move = random.choice(moves)\n",
    "                new_pos = (x + move[0], y + move[1])\n",
    "                if (0 <= new_pos[0] < self.model.grid.width) and (0 <= new_pos[1] < self.model.grid.height):\n",
    "                    if new_pos not in self.model.occupied_positions or new_pos == self.pos:\n",
    "                        self.model.grid.move_agent(self, new_pos)\n",
    "                        self.model.occupied_positions.discard(self.pos)\n",
    "                        self.model.occupied_positions.add(new_pos)\n",
    "                        self.positions.append(new_pos)\n",
    "                        break\n",
    "                attempts += 1\n",
    "            else:\n",
    "                self.positions.append(self.pos)\n",
    "\n",
    "# TimeModel\n",
    "class TimeModel(Model):\n",
    "    def __init__(self, use_rule=True):\n",
    "        super().__init__()\n",
    "        self.grid = MultiGrid(10, 10, False)\n",
    "        self.schedule = []\n",
    "        self.random = random.Random()\n",
    "        self.step_count = 0\n",
    "        self.occupied_positions = set()\n",
    "        self.use_rule = use_rule\n",
    "        \n",
    "        available_positions = [(x, y) for x in range(10) for y in range(10)]\n",
    "        self.random.shuffle(available_positions)\n",
    "        for i in range(30):\n",
    "            agent = TimeAgent(self, use_rule=self.use_rule)\n",
    "            start_pos = available_positions[i]\n",
    "            self.grid.place_agent(agent, start_pos)\n",
    "            agent.pos = start_pos\n",
    "            agent.positions.append(start_pos)\n",
    "            self.occupied_positions.add(start_pos)\n",
    "            self.schedule.append(agent)\n",
    "\n",
    "    def step(self):\n",
    "        self.occupied_positions.clear()\n",
    "        for agent in self.schedule:\n",
    "            self.occupied_positions.add(agent.pos)\n",
    "        random.shuffle(self.schedule)\n",
    "        for agent in self.schedule:\n",
    "            agent.step()\n",
    "        self.step_count += 1\n",
    "\n",
    "    def get_positions(self):\n",
    "        sorted_agents = sorted(self.schedule, key=lambda a: a.unique_id)\n",
    "        return [(agent.positions, [a.pos for a in sorted_agents if a != agent]) for agent in sorted_agents]\n",
    "\n",
    "# TCN\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size=60, output_size=5, num_channels=[128, 128, 128], kernel_size=7, dropout=0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(num_channels)):\n",
    "            dilation = 2 ** i\n",
    "            in_channels = input_size if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            layers.append(nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            if padding > 0:\n",
    "                layers.append(nn.ConstantPad1d((-padding, 0), 0))\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        out = self.tcn(x)\n",
    "        out = out[:, :, -1]\n",
    "        return out\n",
    "\n",
    "# Data collection\n",
    "def collect_and_save_data(num_runs=2000, filename=\"abm_data_with_rule_2.pkl\", use_rule=True):\n",
    "    all_data = []\n",
    "    for run in range(num_runs):\n",
    "        model = TimeModel(use_rule=use_rule)\n",
    "        positions_history = []\n",
    "        for _ in range(10):\n",
    "            model.step()\n",
    "            positions_history.append(model.get_positions())\n",
    "        all_data.append(positions_history)\n",
    "        if (run + 1) % 100 == 0:\n",
    "            print(f\"Completed {run + 1}/{num_runs} runs\")\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(all_data, f)\n",
    "    print(f\"Saved {len(all_data)} runs to {filename}\")\n",
    "    return all_data\n",
    "\n",
    "# Prepare data\n",
    "def prepare_training_data(data, seq_len=5):\n",
    "    X, y = [], []\n",
    "    direction_map = {(0, 1): 0, (0, -1): 1, (1, 0): 2, (-1, 0): 3, (0, 0): 4}\n",
    "    for run_data in data:\n",
    "        for step_idx in range(len(run_data) - seq_len):\n",
    "            step_data = run_data[step_idx:step_idx + seq_len]\n",
    "            for positions, others in step_data[-1]:\n",
    "                if len(positions) < seq_len + 1:\n",
    "                    continue\n",
    "                seq = positions[-seq_len - 1:-1]\n",
    "                seq_data = []\n",
    "                for pos in seq:\n",
    "                    step_features = list(pos)\n",
    "                    for ox, oy in others:\n",
    "                        step_features.extend([ox - pos[0], oy - pos[1]])\n",
    "                    while len(step_features) < 60:\n",
    "                        step_features.extend([0, 0])\n",
    "                    seq_data.append(step_features[:60])\n",
    "                X.append(seq_data)\n",
    "                x1, y1 = positions[-2]\n",
    "                x2, y2 = positions[-1]\n",
    "                direction = (x2 - x1, y2 - y1)\n",
    "                y.append(direction_map[direction])\n",
    "    X = torch.tensor(X, dtype=torch.float32) / 9.0\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    return X, y\n",
    "\n",
    "# Training\n",
    "def train_tcn(X, y, epochs=300, batch_size=32, learning_rate=0.001, patience=20):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = TCN(input_size=60, output_size=5, num_channels=[128, 128, 128], kernel_size=7, dropout=0.2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == batch_y).sum().item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                val_correct += (outputs.argmax(dim=1) == batch_y).sum().item()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        train_acc = train_correct / len(X_train)\n",
    "        val_acc = val_correct / len(X_val)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss_avg:.4f}, Val Loss: {val_loss_avg:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    torch.save(model.state_dict(), \"tcn_model_2.pth\")\n",
    "    print(\"Model saved to tcn_model_2.pth\")\n",
    "    return model\n",
    "\n",
    "# Global variables\n",
    "trained_model = None\n",
    "direction_map = {0: \"Up (0, 1)\", 1: \"Down (0, -1)\", 2: \"Right (1, 0)\", 3: \"Left (-1, 0)\", 4: \"No move (0, 0)\"}\n",
    "reverse_map = {(0, 1): 0, (0, -1): 1, (1, 0): 2, (-1, 0): 3, (0, 0): 4}\n",
    "\n",
    "# Train with rule\n",
    "sequences_with_rule = collect_and_save_data(num_runs=2000, filename=\"abm_data_with_rule_2.pkl\", use_rule=True)\n",
    "X, y = prepare_training_data(sequences_with_rule)\n",
    "print(f\"Loaded {len(sequences_with_rule)} runs with rule\")\n",
    "print(f\"Training data shape: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "print(\"\\nTraining TCN (Direction Prediction with Rule)...\")\n",
    "trained_model = train_tcn(X, y, epochs=300, batch_size=32, learning_rate=0.001, patience=20)\n",
    "\n",
    "# Compare with vs. without rule\n",
    "def compare_rule_effects(runs=100, steps=10):\n",
    "    no_rule_model = TimeModel(use_rule=False)\n",
    "    no_rule_moves = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    for _ in range(runs):\n",
    "        for _ in range(steps):\n",
    "            no_rule_model.step()\n",
    "        for agent in no_rule_model.schedule:\n",
    "            for i in range(len(agent.positions) - 1):\n",
    "                dx, dy = agent.positions[i + 1][0] - agent.positions[i][0], agent.positions[i + 1][1] - agent.positions[i][1]\n",
    "                move_idx = reverse_map.get((dx, dy), 4)\n",
    "                no_rule_moves[move_idx] += 1\n",
    "\n",
    "    with_rule_model = TimeModel(use_rule=True)\n",
    "    with_rule_moves = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    for _ in range(runs):\n",
    "        for _ in range(steps):\n",
    "            with_rule_model.step()\n",
    "        for agent in with_rule_model.schedule:\n",
    "            for i in range(len(agent.positions) - 1):\n",
    "                dx, dy = agent.positions[i + 1][0] - agent.positions[i][0], agent.positions[i + 1][1] - agent.positions[i][1]\n",
    "                move_idx = reverse_map.get((dx, dy), 4)\n",
    "                with_rule_moves[move_idx] += 1\n",
    "\n",
    "    total_no_rule = sum(no_rule_moves.values())\n",
    "    total_with_rule = sum(with_rule_moves.values())\n",
    "    print(\"\\nNo Rule Move Frequencies:\", {direction_map[k]: v/total_no_rule for k, v in no_rule_moves.items()})\n",
    "    print(\"With Rule Move Frequencies:\", {direction_map[k]: v/total_with_rule for k, v in with_rule_moves.items()})\n",
    "\n",
    "# Run comparison\n",
    "compare_rule_effects(runs=100, steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add rule: center bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100/2000 runs\n",
      "Completed 200/2000 runs\n",
      "Completed 300/2000 runs\n",
      "Completed 400/2000 runs\n",
      "Completed 500/2000 runs\n",
      "Completed 600/2000 runs\n",
      "Completed 700/2000 runs\n",
      "Completed 800/2000 runs\n",
      "Completed 900/2000 runs\n",
      "Completed 1000/2000 runs\n",
      "Completed 1100/2000 runs\n",
      "Completed 1200/2000 runs\n",
      "Completed 1300/2000 runs\n",
      "Completed 1400/2000 runs\n",
      "Completed 1500/2000 runs\n",
      "Completed 1600/2000 runs\n",
      "Completed 1700/2000 runs\n",
      "Completed 1800/2000 runs\n",
      "Completed 1900/2000 runs\n",
      "Completed 2000/2000 runs\n",
      "Saved 2000 runs to abm_data_with_rule_add.pkl\n",
      "Loaded 2000 runs with rule\n",
      "Training data shape: X=torch.Size([360000, 5, 60]), y=torch.Size([360000])\n",
      "\n",
      "Training TCN (Direction Prediction with Rule)...\n",
      "Epoch [10/300], Train Loss: 3.9247, Val Loss: 3.6444, Train Acc: 0.2626, Val Acc: 0.2801, LR: 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 270\u001b[39m\n\u001b[32m    267\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining data shape: X=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, y=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    269\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining TCN (Direction Prediction with Rule)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m trained_model = \u001b[43mtrain_tcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# Compare with vs. without rule\u001b[39;00m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompare_rule_effects\u001b[39m(runs=\u001b[32m100\u001b[39m, steps=\u001b[32m10\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 218\u001b[39m, in \u001b[36mtrain_tcn\u001b[39m\u001b[34m(X, y, epochs, batch_size, learning_rate, patience)\u001b[39m\n\u001b[32m    216\u001b[39m outputs = model(batch_X)\n\u001b[32m    217\u001b[39m loss = criterion(outputs, batch_y)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m optimizer.step()\n\u001b[32m    220\u001b[39m train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fengyong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fengyong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fengyong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import random\n",
    "from mesa import Agent, Model\n",
    "from mesa.space import MultiGrid\n",
    "\n",
    "# TimeAgent with Center Bias\n",
    "class TimeAgent(Agent):\n",
    "    def __init__(self, model, use_rule=True):\n",
    "        super().__init__(model)\n",
    "        self.model = model\n",
    "        self.positions = []\n",
    "        self.use_rule = use_rule\n",
    "\n",
    "    def step(self):\n",
    "        global trained_model\n",
    "        x, y = self.pos\n",
    "        moves = [(0, 1), (-1, 0), (0, -1), (1, 0)]\n",
    "        \n",
    "        avoid_move = None\n",
    "        if len(self.positions) >= 5 and trained_model is not None and self.use_rule:\n",
    "            seq = self.positions[-5:]\n",
    "            others = [a.pos for a in self.model.schedule if a != self]\n",
    "            input_data = []\n",
    "            for pos in seq:\n",
    "                step_features = list(pos)\n",
    "                for ox, oy in others:\n",
    "                    step_features.extend([ox - pos[0], oy - pos[1]])\n",
    "                while len(step_features) < 60:\n",
    "                    step_features.extend([0, 0])\n",
    "                input_data.append(step_features[:60])\n",
    "            input_data = torch.tensor([input_data], dtype=torch.float32) / 9.0\n",
    "            with torch.no_grad():\n",
    "                pred_dir = trained_model(input_data).argmax().item()\n",
    "            avoid_move = list(reverse_map.keys())[pred_dir]\n",
    "\n",
    "        attempts = 0\n",
    "        max_attempts = 10\n",
    "        move_idx = random.randint(0, 3)\n",
    "\n",
    "        if self.use_rule:\n",
    "            while attempts < max_attempts:\n",
    "                move = moves[move_idx]\n",
    "                if move == avoid_move and attempts < max_attempts - 1:\n",
    "                    move_idx = (move_idx + 1) % 4\n",
    "                    attempts += 1\n",
    "                    continue\n",
    "                new_pos = (x + move[0], y + move[1])\n",
    "                if (0 <= new_pos[0] < self.model.grid.width) and (0 <= new_pos[1] < self.model.grid.height):\n",
    "                    if new_pos not in self.model.occupied_positions or new_pos == self.pos:\n",
    "                        self.model.grid.move_agent(self, new_pos)\n",
    "                        self.model.occupied_positions.discard(self.pos)\n",
    "                        self.model.occupied_positions.add(new_pos)\n",
    "                        self.positions.append(new_pos)\n",
    "                        break\n",
    "\n",
    "                # Turn left if occupied, else bias toward center (5, 5)\n",
    "                center_x, center_y = 5, 5\n",
    "                if x < center_x and (1, 0) not in self.model.occupied_positions:\n",
    "                    move_idx = 2   # Right\n",
    "                elif x > center_x and (-1, 0) not in self.model.occupied_positions:\n",
    "                    move_idx = 1   # Left\n",
    "                elif y < center_y and (0, 1) not in self.model.occupied_positions:\n",
    "                    move_idx = 0   # Up\n",
    "                elif y > center_y and (0, -1) not in self.model.occupied_positions:\n",
    "                    move_idx = 3   # Down\n",
    "                else:\n",
    "                    move_idx = (move_idx + 1) % 4\n",
    "                attempts += 1\n",
    "            else:\n",
    "                self.positions.append(self.pos)\n",
    "        else:\n",
    "            while attempts < max_attempts:\n",
    "                move = random.choice(moves)\n",
    "                new_pos = (x + move[0], y + move[1])\n",
    "                if (0 <= new_pos[0] < self.model.grid.width) and (0 <= new_pos[1] < self.model.grid.height):\n",
    "                    if new_pos not in self.model.occupied_positions or new_pos == self.pos:\n",
    "                        self.model.grid.move_agent(self, new_pos)\n",
    "                        self.model.occupied_positions.discard(self.pos)\n",
    "                        self.model.occupied_positions.add(new_pos)\n",
    "                        self.positions.append(new_pos)\n",
    "                        break\n",
    "                attempts += 1\n",
    "            else:\n",
    "                self.positions.append(self.pos)\n",
    "\n",
    "# TimeModel\n",
    "class TimeModel(Model):\n",
    "    def __init__(self, use_rule=True):\n",
    "        super().__init__()\n",
    "        self.grid = MultiGrid(10, 10, False)\n",
    "        self.schedule = []\n",
    "        self.random = random.Random()\n",
    "        self.step_count = 0\n",
    "        self.occupied_positions = set()\n",
    "        self.use_rule = use_rule\n",
    "        \n",
    "        available_positions = [(x, y) for x in range(10) for y in range(10)]\n",
    "        self.random.shuffle(available_positions)\n",
    "        for i in range(30):\n",
    "            agent = TimeAgent(self, use_rule=self.use_rule)\n",
    "            start_pos = available_positions[i]\n",
    "            self.grid.place_agent(agent, start_pos)\n",
    "            agent.pos = start_pos\n",
    "            agent.positions.append(start_pos)\n",
    "            self.occupied_positions.add(start_pos)\n",
    "            self.schedule.append(agent)\n",
    "\n",
    "    def step(self):\n",
    "        self.occupied_positions.clear()\n",
    "        for agent in self.schedule:\n",
    "            self.occupied_positions.add(agent.pos)\n",
    "        random.shuffle(self.schedule)\n",
    "        for agent in self.schedule:\n",
    "            agent.step()\n",
    "        self.step_count += 1\n",
    "\n",
    "    def get_positions(self):\n",
    "        sorted_agents = sorted(self.schedule, key=lambda a: a.unique_id)\n",
    "        return [(agent.positions, [a.pos for a in sorted_agents if a != agent]) for agent in sorted_agents]\n",
    "\n",
    "# TCN\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size=60, output_size=5, num_channels=[256, 256, 256], kernel_size=9, dropout=0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(num_channels)):\n",
    "            dilation = 2 ** i\n",
    "            in_channels = input_size if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            layers.append(nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            if padding > 0:\n",
    "                layers.append(nn.ConstantPad1d((-padding, 0), 0))\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        out = self.tcn(x)\n",
    "        out = out[:, :, -1]\n",
    "        return out\n",
    "\n",
    "# Data collection (10 steps)\n",
    "def collect_and_save_data(num_runs=2000, filename=\"abm_data_with_rule_add.pkl\", use_rule=True):\n",
    "    all_data = []\n",
    "    for run in range(num_runs):\n",
    "        model = TimeModel(use_rule=use_rule)\n",
    "        positions_history = []\n",
    "        for _ in range(10):\n",
    "            model.step()\n",
    "            positions_history.append(model.get_positions())\n",
    "        all_data.append(positions_history)\n",
    "        if (run + 1) % 100 == 0:\n",
    "            print(f\"Completed {run + 1}/{num_runs} runs\")\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(all_data, f)\n",
    "    print(f\"Saved {len(all_data)} runs to {filename}\")\n",
    "    return all_data\n",
    "\n",
    "# Prepare data (Optimized for 10 steps)\n",
    "def prepare_training_data(data, seq_len=5):\n",
    "    X, y = [], []\n",
    "    direction_map = {(0, 1): 0, (0, -1): 1, (1, 0): 2, (-1, 0): 3, (0, 0): 4}\n",
    "    for run_data in data:\n",
    "        for step_idx in range(len(run_data) - seq_len + 1):  # Maximize windows: 10 - 5 + 1 = 6\n",
    "            step_data = run_data[step_idx:step_idx + seq_len]\n",
    "            for positions, others in step_data[-1]:\n",
    "                if len(positions) < step_idx + seq_len + 1:  # Ensure enough steps\n",
    "                    continue\n",
    "                seq = positions[step_idx:step_idx + seq_len]  # 5-step window\n",
    "                seq_data = []\n",
    "                for pos in seq:\n",
    "                    step_features = list(pos)\n",
    "                    for ox, oy in others:\n",
    "                        step_features.extend([ox - pos[0], oy - pos[1]])\n",
    "                    while len(step_features) < 60:\n",
    "                        step_features.extend([0, 0])\n",
    "                    seq_data.append(step_features[:60])\n",
    "                X.append(seq_data)\n",
    "                x1, y1 = positions[step_idx + seq_len - 1]  # Last of seq\n",
    "                x2, y2 = positions[step_idx + seq_len]      # Target\n",
    "                direction = (x2 - x1, y2 - y1)\n",
    "                y.append(direction_map[direction])\n",
    "    X = torch.tensor(X, dtype=torch.float32) / 9.0\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    return X, y\n",
    "\n",
    "# Training\n",
    "def train_tcn(X, y, epochs=300, batch_size=32, learning_rate=0.001, patience=20):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = TCN(input_size=60, output_size=5, num_channels=[256, 256, 256], kernel_size=9, dropout=0.2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == batch_y).sum().item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                val_correct += (outputs.argmax(dim=1) == batch_y).sum().item()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        train_acc = train_correct / len(X_train)\n",
    "        val_acc = val_correct / len(X_val)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss_avg:.4f}, Val Loss: {val_loss_avg:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    torch.save(model.state_dict(), \"tcn_model_add.pth\")\n",
    "    print(\"Model saved to tcn_model_add.pth\")\n",
    "    return model\n",
    "\n",
    "# Global variables\n",
    "trained_model = None\n",
    "direction_map = {0: \"Up (0, 1)\", 1: \"Down (0, -1)\", 2: \"Right (1, 0)\", 3: \"Left (-1, 0)\", 4: \"No move (0, 0)\"}\n",
    "reverse_map = {(0, 1): 0, (0, -1): 1, (1, 0): 2, (-1, 0): 3, (0, 0): 4}\n",
    "\n",
    "# Train with rule\n",
    "sequences_with_rule = collect_and_save_data(num_runs=2000, filename=\"abm_data_with_rule_add.pkl\", use_rule=True)\n",
    "X, y = prepare_training_data(sequences_with_rule)\n",
    "print(f\"Loaded {len(sequences_with_rule)} runs with rule\")\n",
    "print(f\"Training data shape: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "print(\"\\nTraining TCN (Direction Prediction with Rule)...\")\n",
    "trained_model = train_tcn(X, y, epochs=300, batch_size=32, learning_rate=0.001, patience=20)\n",
    "\n",
    "# Compare with vs. without rule\n",
    "def compare_rule_effects(runs=100, steps=10):\n",
    "    no_rule_model = TimeModel(use_rule=False)\n",
    "    no_rule_moves = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    for _ in range(runs):\n",
    "        for _ in range(steps):\n",
    "            no_rule_model.step()\n",
    "        for agent in no_rule_model.schedule:\n",
    "            for i in range(len(agent.positions) - 1):\n",
    "                dx, dy = agent.positions[i + 1][0] - agent.positions[i][0], agent.positions[i + 1][1] - agent.positions[i][1]\n",
    "                move_idx = reverse_map.get((dx, dy), 4)\n",
    "                no_rule_moves[move_idx] += 1\n",
    "\n",
    "    with_rule_model = TimeModel(use_rule=True)\n",
    "    with_rule_moves = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    for _ in range(runs):\n",
    "        for _ in range(steps):\n",
    "            with_rule_model.step()\n",
    "        for agent in with_rule_model.schedule:\n",
    "            for i in range(len(agent.positions) - 1):\n",
    "                dx, dy = agent.positions[i + 1][0] - agent.positions[i][0], agent.positions[i + 1][1] - agent.positions[i][1]\n",
    "                move_idx = reverse_map.get((dx, dy), 4)\n",
    "                with_rule_moves[move_idx] += 1\n",
    "\n",
    "    total_no_rule = sum(no_rule_moves.values())\n",
    "    total_with_rule = sum(with_rule_moves.values())\n",
    "    print(\"\\nNo Rule Move Frequencies:\", {direction_map[k]: v/total_no_rule for k, v in no_rule_moves.items()})\n",
    "    print(\"With Rule Move Frequencies:\", {direction_map[k]: v/total_with_rule for k, v in with_rule_moves.items()})\n",
    "\n",
    "# Run comparison\n",
    "compare_rule_effects(runs=100, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import random\n",
    "from mesa import Agent, Model\n",
    "from mesa.space import MultiGrid\n",
    "\n",
    "# TimeAgent with Center Bias\n",
    "class TimeAgent(Agent):\n",
    "    def __init__(self, model, use_rule=True):\n",
    "        super().__init__(model)\n",
    "        self.model = model\n",
    "        self.positions = []\n",
    "        self.use_rule = use_rule\n",
    "\n",
    "    def step(self):\n",
    "        global trained_model\n",
    "        x, y = self.pos\n",
    "        moves = [(0, 1), (-1, 0), (0, -1), (1, 0)]\n",
    "        \n",
    "        avoid_move = None\n",
    "        if len(self.positions) >= 5 and trained_model is not None and self.use_rule:\n",
    "            seq = self.positions[-5:]\n",
    "            others = [a.pos for a in self.model.schedule if a != self]\n",
    "            input_data = []\n",
    "            for pos in seq:\n",
    "                step_features = list(pos)\n",
    "                for ox, oy in others:\n",
    "                    step_features.extend([ox - pos[0], oy - pos[1]])\n",
    "                while len(step_features) < 60:\n",
    "                    step_features.extend([0, 0])\n",
    "                input_data.append(step_features[:60])\n",
    "            input_data = torch.tensor([input_data], dtype=torch.float32) / 9.0\n",
    "            with torch.no_grad():\n",
    "                pred_dir = trained_model(input_data).argmax().item()\n",
    "            avoid_move = list(reverse_map.keys())[pred_dir]\n",
    "\n",
    "        attempts = 0\n",
    "        max_attempts = 10\n",
    "        move_idx = random.randint(0, 3)\n",
    "\n",
    "        if self.use_rule:\n",
    "            while attempts < max_attempts:\n",
    "                move = moves[move_idx]\n",
    "                if move == avoid_move and attempts < max_attempts - 1:\n",
    "                    move_idx = (move_idx + 1) % 4\n",
    "                    attempts += 1\n",
    "                    continue\n",
    "                new_pos = (x + move[0], y + move[1])\n",
    "                if (0 <= new_pos[0] < self.model.grid.width) and (0 <= new_pos[1] < self.model.grid.height):\n",
    "                    if new_pos not in self.model.occupied_positions or new_pos == self.pos:\n",
    "                        self.model.grid.move_agent(self, new_pos)\n",
    "                        self.model.occupied_positions.discard(self.pos)\n",
    "                        self.model.occupied_positions.add(new_pos)\n",
    "                        self.positions.append(new_pos)\n",
    "                        break\n",
    "                center_x, center_y = 5, 5\n",
    "                if x < center_x and (1, 0) not in self.model.occupied_positions:\n",
    "                    move_idx = 2\n",
    "                elif x > center_x and (-1, 0) not in self.model.occupied_positions:\n",
    "                    move_idx = 1\n",
    "                elif y < center_y and (0, 1) not in self.model.occupied_positions:\n",
    "                    move_idx = 0\n",
    "                elif y > center_y and (0, -1) not in self.model.occupied_positions:\n",
    "                    move_idx = 3\n",
    "                else:\n",
    "                    move_idx = (move_idx + 1) % 4\n",
    "                attempts += 1\n",
    "            else:\n",
    "                self.positions.append(self.pos)\n",
    "        else:\n",
    "            while attempts < max_attempts:\n",
    "                move = random.choice(moves)\n",
    "                new_pos = (x + move[0], y + move[1])\n",
    "                if (0 <= new_pos[0] < self.model.grid.width) and (0 <= new_pos[1] < self.model.grid.height):\n",
    "                    if new_pos not in self.model.occupied_positions or new_pos == self.pos:\n",
    "                        self.model.grid.move_agent(self, new_pos)\n",
    "                        self.model.occupied_positions.discard(self.pos)\n",
    "                        self.model.occupied_positions.add(new_pos)\n",
    "                        self.positions.append(new_pos)\n",
    "                        break\n",
    "                attempts += 1\n",
    "            else:\n",
    "                self.positions.append(self.pos)\n",
    "\n",
    "# TimeModel\n",
    "class TimeModel(Model):\n",
    "    def __init__(self, use_rule=True):\n",
    "        super().__init__()\n",
    "        self.grid = MultiGrid(10, 10, False)\n",
    "        self.schedule = []\n",
    "        self.random = random.Random()\n",
    "        self.step_count = 0\n",
    "        self.occupied_positions = set()\n",
    "        self.use_rule = use_rule\n",
    "        \n",
    "        available_positions = [(x, y) for x in range(10) for y in range(10)]\n",
    "        self.random.shuffle(available_positions)\n",
    "        for i in range(30):\n",
    "            agent = TimeAgent(self, use_rule=self.use_rule)\n",
    "            start_pos = available_positions[i]\n",
    "            self.grid.place_agent(agent, start_pos)\n",
    "            agent.pos = start_pos\n",
    "            agent.positions.append(start_pos)\n",
    "            self.occupied_positions.add(start_pos)\n",
    "            self.schedule.append(agent)\n",
    "\n",
    "    def step(self):\n",
    "        self.occupied_positions.clear()\n",
    "        for agent in self.schedule:\n",
    "            self.occupied_positions.add(agent.pos)\n",
    "        random.shuffle(self.schedule)\n",
    "        for agent in self.schedule:\n",
    "            agent.step()\n",
    "        self.step_count += 1\n",
    "\n",
    "    def get_positions(self):\n",
    "        sorted_agents = sorted(self.schedule, key=lambda a: a.unique_id)\n",
    "        return [(agent.positions, [a.pos for a in sorted_agents if a != agent]) for agent in sorted_agents]\n",
    "\n",
    "# TCN\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size=60, output_size=5, num_channels=[128, 128, 128], kernel_size=7, dropout=0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(num_channels)):\n",
    "            dilation = 2 ** i\n",
    "            in_channels = input_size if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            layers.append(nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            if padding > 0:\n",
    "                layers.append(nn.ConstantPad1d((-padding, 0), 0))\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        out = self.tcn(x)\n",
    "        out = out[:, :, -1]\n",
    "        return out\n",
    "\n",
    "# Data collection (10 steps)\n",
    "def collect_and_save_data(num_runs=2000, filename=\"abm_data_with_rule_3.pkl\", use_rule=True):\n",
    "    all_data = []\n",
    "    for run in range(num_runs):\n",
    "        model = TimeModel(use_rule=use_rule)\n",
    "        positions_history = []\n",
    "        for _ in range(10):\n",
    "            model.step()\n",
    "            positions_history.append(model.get_positions())\n",
    "        all_data.append(positions_history)\n",
    "        if (run + 1) % 100 == 0:\n",
    "            print(f\"Completed {run + 1}/{num_runs} runs\")\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(all_data, f)\n",
    "    print(f\"Saved {len(all_data)} runs to {filename}\")\n",
    "    return all_data\n",
    "\n",
    "# Prepare data\n",
    "def prepare_training_data(data, seq_len=5):\n",
    "    X, y = [], []\n",
    "    direction_map = {(0, 1): 0, (0, -1): 1, (1, 0): 2, (-1, 0): 3, (0, 0): 4}\n",
    "    for run_data in data:\n",
    "        for step_idx in range(len(run_data) - seq_len + 1):\n",
    "            step_data = run_data[step_idx:step_idx + seq_len]\n",
    "            for positions, others in step_data[-1]:\n",
    "                if len(positions) < step_idx + seq_len + 1:\n",
    "                    continue\n",
    "                seq = positions[step_idx:step_idx + seq_len]\n",
    "                seq_data = []\n",
    "                for pos in seq:\n",
    "                    step_features = list(pos)\n",
    "                    for ox, oy in others:\n",
    "                        step_features.extend([ox - pos[0], oy - pos[1]])\n",
    "                    while len(step_features) < 60:\n",
    "                        step_features.extend([0, 0])\n",
    "                    seq_data.append(step_features[:60])\n",
    "                X.append(seq_data)\n",
    "                x1, y1 = positions[step_idx + seq_len - 1]\n",
    "                x2, y2 = positions[step_idx + seq_len]\n",
    "                direction = (x2 - x1, y2 - y1)\n",
    "                y.append(direction_map[direction])\n",
    "    X = torch.tensor(X, dtype=torch.float32) / 9.0\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    return X, y\n",
    "\n",
    "# Training\n",
    "def train_tcn(X, y, epochs=300, batch_size=64, learning_rate=0.001, patience=20):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = TCN(input_size=60, output_size=5, num_channels=[128, 128, 128], kernel_size=7, dropout=0.2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == batch_y).sum().item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                val_correct += (outputs.argmax(dim=1) == batch_y).sum().item()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        train_acc = train_correct / len(X_train)\n",
    "        val_acc = val_correct / len(X_val)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss_avg:.4f}, Val Loss: {val_loss_avg:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    torch.save(model.state_dict(), \"tcn_model_3.pth\")\n",
    "    print(\"Model saved to tcn_model_3.pth\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100/2000 runs\n",
      "Completed 200/2000 runs\n",
      "Completed 300/2000 runs\n",
      "Completed 400/2000 runs\n",
      "Completed 500/2000 runs\n",
      "Completed 600/2000 runs\n",
      "Completed 700/2000 runs\n",
      "Completed 800/2000 runs\n",
      "Completed 900/2000 runs\n",
      "Completed 1000/2000 runs\n",
      "Completed 1100/2000 runs\n",
      "Completed 1200/2000 runs\n",
      "Completed 1300/2000 runs\n",
      "Completed 1400/2000 runs\n",
      "Completed 1500/2000 runs\n",
      "Completed 1600/2000 runs\n",
      "Completed 1700/2000 runs\n",
      "Completed 1800/2000 runs\n",
      "Completed 1900/2000 runs\n",
      "Completed 2000/2000 runs\n",
      "Saved 2000 runs to abm_data_with_rule_3.pkl\n",
      "Loaded 2000 runs with rule\n",
      "Training data shape: X=torch.Size([360000, 5, 60]), y=torch.Size([360000])\n",
      "\n",
      "Training TCN (Direction Prediction with Rule)...\n",
      "Epoch [10/300], Train Loss: 3.0290, Val Loss: 2.6938, Train Acc: 0.2859, Val Acc: 0.2952, LR: 0.001000\n",
      "Epoch [20/300], Train Loss: 3.0124, Val Loss: 2.6229, Train Acc: 0.2934, Val Acc: 0.3073, LR: 0.001000\n",
      "Epoch [30/300], Train Loss: 3.0106, Val Loss: 2.6400, Train Acc: 0.2961, Val Acc: 0.3086, LR: 0.001000\n",
      "Epoch [40/300], Train Loss: 3.0064, Val Loss: 2.6332, Train Acc: 0.2977, Val Acc: 0.3102, LR: 0.001000\n",
      "Epoch [50/300], Train Loss: 2.9959, Val Loss: 2.6343, Train Acc: 0.3016, Val Acc: 0.3123, LR: 0.000500\n",
      "Early stopping triggered after 52 epochs\n",
      "Model saved to tcn_model_3.pth\n",
      "\n",
      "No Rule Move Frequencies: {'Up (0, 1)': 0.24393663366336635, 'Down (0, -1)': 0.24435973597359736, 'Right (1, 0)': 0.24489042904290428, 'Left (-1, 0)': 0.24451881188118813, 'No move (0, 0)': 0.022294389438943893}\n",
      "With Rule Move Frequencies: {'Up (0, 1)': 0.15204026402640264, 'Down (0, -1)': 0.15486468646864687, 'Right (1, 0)': 0.18327062706270628, 'Left (-1, 0)': 0.18066468646864686, 'No move (0, 0)': 0.3291597359735974}\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "trained_model = None\n",
    "direction_map = {0: \"Up (0, 1)\", 1: \"Down (0, -1)\", 2: \"Right (1, 0)\", 3: \"Left (-1, 0)\", 4: \"No move (0, 0)\"}\n",
    "reverse_map = {(0, 1): 0, (0, -1): 1, (1, 0): 2, (-1, 0): 3, (0, 0): 4}\n",
    "\n",
    "# Train with rule\n",
    "sequences_with_rule = collect_and_save_data(num_runs=2000, filename=\"abm_data_with_rule_3.pkl\", use_rule=True)\n",
    "X, y = prepare_training_data(sequences_with_rule)\n",
    "print(f\"Loaded {len(sequences_with_rule)} runs with rule\")\n",
    "print(f\"Training data shape: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "print(\"\\nTraining TCN (Direction Prediction with Rule)...\")\n",
    "trained_model = train_tcn(X, y, epochs=300, batch_size=64, learning_rate=0.001, patience=20)\n",
    "\n",
    "# Compare with vs. without rule\n",
    "def compare_rule_effects(runs=100, steps=10):\n",
    "    no_rule_model = TimeModel(use_rule=False)\n",
    "    no_rule_moves = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    for _ in range(runs):\n",
    "        for _ in range(steps):\n",
    "            no_rule_model.step()\n",
    "        for agent in no_rule_model.schedule:\n",
    "            for i in range(len(agent.positions) - 1):\n",
    "                dx, dy = agent.positions[i + 1][0] - agent.positions[i][0], agent.positions[i + 1][1] - agent.positions[i][1]\n",
    "                move_idx = reverse_map.get((dx, dy), 4)\n",
    "                no_rule_moves[move_idx] += 1\n",
    "\n",
    "    with_rule_model = TimeModel(use_rule=True)\n",
    "    with_rule_moves = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    for _ in range(runs):\n",
    "        for _ in range(steps):\n",
    "            with_rule_model.step()\n",
    "        for agent in with_rule_model.schedule:\n",
    "            for i in range(len(agent.positions) - 1):\n",
    "                dx, dy = agent.positions[i + 1][0] - agent.positions[i][0], agent.positions[i + 1][1] - agent.positions[i][1]\n",
    "                move_idx = reverse_map.get((dx, dy), 4)\n",
    "                with_rule_moves[move_idx] += 1\n",
    "\n",
    "    total_no_rule = sum(no_rule_moves.values())\n",
    "    total_with_rule = sum(with_rule_moves.values())\n",
    "    print(\"\\nNo Rule Move Frequencies:\", {direction_map[k]: v/total_no_rule for k, v in no_rule_moves.items()})\n",
    "    print(\"With Rule Move Frequencies:\", {direction_map[k]: v/total_with_rule for k, v in with_rule_moves.items()})\n",
    "\n",
    "# Run comparison\n",
    "compare_rule_effects(runs=100, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100/1000 runs\n",
      "Completed 200/1000 runs\n",
      "Completed 300/1000 runs\n",
      "Completed 400/1000 runs\n",
      "Completed 500/1000 runs\n",
      "Completed 600/1000 runs\n",
      "Completed 700/1000 runs\n",
      "Completed 800/1000 runs\n",
      "Completed 900/1000 runs\n",
      "Completed 1000/1000 runs\n",
      "Saved 5000 sequences to abm_data.pkl\n",
      "Loaded 5000 sequences\n",
      "Training data shape: X=torch.Size([4998, 5, 2]), y=torch.Size([4998])\n",
      "Sample sequence 1: [(6, 3), (5, 3), (5, 2), (5, 3), (5, 2), (6, 2)]\n",
      "Sample sequence 2: [(8, 6), (8, 7), (9, 7), (8, 7), (8, 6), (8, 7)]\n",
      "Sample sequence 3: [(0, 4), (0, 3), (0, 4), (1, 4), (1, 5), (1, 4)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "def collect_and_save_data(num_runs=1000, filename=\"abm_data.pkl\"):\n",
    "    all_sequences = []\n",
    "    for run in range(num_runs):\n",
    "        model = TimeModel()\n",
    "        for _ in range(5):\n",
    "            model.step()    \n",
    "        sequences = model.get_positions()\n",
    "        all_sequences.extend(sequences)\n",
    "        if (run + 1) % 100 == 0:  # Print progress every 10 runs\n",
    "            print(f\"Completed {run + 1}/{num_runs} runs\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(all_sequences, f)\n",
    "    print(f\"Saved {len(all_sequences)} sequences to {filename}\")\n",
    "    return all_sequences\n",
    "\n",
    "def load_data(filename=\"abm_data.pkl\"):\n",
    "    with open(filename, 'rb') as f:\n",
    "        sequences = pickle.load(f)\n",
    "    return sequences\n",
    "'''\n",
    "def prepare_training_data(sequences, seq_len=5):\n",
    "    X, y = [], []\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= seq_len + 1:  # Ensure enough positions\n",
    "            X.append(seq[:seq_len])  # Input: first 5 positions\n",
    "            y.append(seq[seq_len])   # Target: 6th position\n",
    "    X = torch.tensor(X, dtype=torch.float32)  # Shape: (num_samples, seq_len, 2)\n",
    "    y = torch.tensor(y, dtype=torch.float32)  # Shape: (num_samples, 2)\n",
    "    return X, y\n",
    "'''\n",
    "\n",
    "# Updated prepare_training_data for classification\n",
    "def prepare_training_data(sequences, seq_len=5):\n",
    "    X, y = [], []\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= seq_len + 1:\n",
    "            X.append(seq[:seq_len])\n",
    "            # Convert (x, y) target to grid index: y * 10 + x\n",
    "            x, y_coord = seq[seq_len]\n",
    "            y.append(y_coord * 10 + x)\n",
    "    X = torch.tensor(X, dtype=torch.float32) / 9.0  # Normalize inputs\n",
    "    y = torch.tensor(y, dtype=torch.long)  # Long for class indices\n",
    "    return X, y\n",
    "\n",
    "# Collect and save\n",
    "sequences = collect_and_save_data(num_runs=1000)\n",
    "\n",
    "# Later, load and prepare\n",
    "# sequences = load_data()\n",
    "X, y = prepare_training_data(sequences)\n",
    "print(f\"Loaded {len(sequences)} sequences\")\n",
    "print(f\"Training data shape: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "\n",
    "# Example: Print a few sequences for verification\n",
    "for i in range(min(3, len(sequences))):\n",
    "    print(f\"Sample sequence {i+1}: {sequences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: X=torch.Size([4998, 5, 2]), y=torch.Size([4998])\n",
      "\n",
      "Training BiLSTM (Classification)...\n",
      "Epoch [10/300], Train Loss: 2.9309, Val Loss: 2.9550, LR: 0.001000\n",
      "Epoch [20/300], Train Loss: 2.6277, Val Loss: 2.6621, LR: 0.001000\n",
      "Epoch [30/300], Train Loss: 2.4649, Val Loss: 2.5072, LR: 0.001000\n",
      "Epoch [40/300], Train Loss: 2.3657, Val Loss: 2.4204, LR: 0.001000\n",
      "Epoch [50/300], Train Loss: 2.2955, Val Loss: 2.3561, LR: 0.000500\n",
      "Epoch [60/300], Train Loss: 2.2505, Val Loss: 2.3216, LR: 0.000500\n",
      "Epoch [70/300], Train Loss: 2.2288, Val Loss: 2.3013, LR: 0.000500\n",
      "Epoch [80/300], Train Loss: 2.2087, Val Loss: 2.2836, LR: 0.000500\n",
      "Epoch [90/300], Train Loss: 2.1879, Val Loss: 2.2748, LR: 0.000500\n",
      "Epoch [100/300], Train Loss: 2.1696, Val Loss: 2.2646, LR: 0.000250\n",
      "Epoch [110/300], Train Loss: 2.1539, Val Loss: 2.2613, LR: 0.000250\n",
      "Epoch [120/300], Train Loss: 2.1460, Val Loss: 2.2525, LR: 0.000250\n",
      "Epoch [130/300], Train Loss: 2.1389, Val Loss: 2.2493, LR: 0.000250\n",
      "Epoch [140/300], Train Loss: 2.1315, Val Loss: 2.2448, LR: 0.000250\n",
      "Epoch [150/300], Train Loss: 2.1253, Val Loss: 2.2429, LR: 0.000125\n",
      "Epoch [160/300], Train Loss: 2.1172, Val Loss: 2.2403, LR: 0.000125\n",
      "Epoch [170/300], Train Loss: 2.1142, Val Loss: 2.2384, LR: 0.000125\n",
      "Epoch [180/300], Train Loss: 2.1111, Val Loss: 2.2359, LR: 0.000125\n",
      "Epoch [190/300], Train Loss: 2.1080, Val Loss: 2.2361, LR: 0.000125\n",
      "Epoch [200/300], Train Loss: 2.1047, Val Loss: 2.2352, LR: 0.000063\n",
      "Epoch [210/300], Train Loss: 2.1008, Val Loss: 2.2337, LR: 0.000063\n",
      "Epoch [220/300], Train Loss: 2.0991, Val Loss: 2.2334, LR: 0.000063\n",
      "Epoch [230/300], Train Loss: 2.0974, Val Loss: 2.2325, LR: 0.000063\n",
      "Epoch [240/300], Train Loss: 2.0963, Val Loss: 2.2325, LR: 0.000063\n",
      "Epoch [250/300], Train Loss: 2.0948, Val Loss: 2.2319, LR: 0.000031\n",
      "Epoch [260/300], Train Loss: 2.0926, Val Loss: 2.2316, LR: 0.000031\n",
      "Epoch [270/300], Train Loss: 2.0918, Val Loss: 2.2310, LR: 0.000031\n",
      "Epoch [280/300], Train Loss: 2.0910, Val Loss: 2.2307, LR: 0.000031\n",
      "Epoch [290/300], Train Loss: 2.0905, Val Loss: 2.2306, LR: 0.000031\n",
      "Epoch [300/300], Train Loss: 2.0896, Val Loss: 2.2304, LR: 0.000016\n",
      "Model saved to bilstm_model.pth\n",
      "\n",
      "Sample predictions vs. actual:\n",
      "Sequence 1: [(6, 3), (5, 3), (5, 2), (5, 3), (5, 2), (6, 2)]\n",
      "Predicted next: (5, 2), Actual: (6, 2)\n",
      "Sequence 2: [(8, 6), (8, 7), (9, 7), (8, 7), (8, 6), (8, 7)]\n",
      "Predicted next: (8, 6), Actual: (8, 7)\n",
      "Sequence 3: [(0, 4), (0, 3), (0, 4), (1, 4), (1, 5), (1, 4)]\n",
      "Predicted next: (1, 5), Actual: (1, 4)\n",
      "Sequence 4: [(7, 5), (7, 4), (7, 3), (6, 3), (6, 4), (6, 3)]\n",
      "Predicted next: (6, 4), Actual: (6, 3)\n",
      "Sequence 5: [(2, 7), (2, 8), (3, 8), (4, 8), (4, 7), (4, 6)]\n",
      "Predicted next: (4, 7), Actual: (4, 6)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the BiLSTM model\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=32, output_size=100): # 100 classes for 10x10 grid\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size) # Output: 100 grid positions\n",
    "                                                          # Output size of LSTM is hidden_size * 2 (forward + backward)\n",
    "\n",
    "    def forward(self, x):\n",
    "                               # x shape: (batch_size, seq_len, input_size)\n",
    "        out, _ = self.lstm(x)  # out: (batch_size, seq_len, hidden_size * 2)\n",
    "        out = self.fc(out[:, -1, :])  # Raw logits for classification # Take the last time step: (batch_size, output_size)\n",
    "        return out\n",
    "\n",
    "# Training function with early stopping\n",
    "def train_rnn(X, y, epochs=300, batch_size=32, learning_rate=0.001, patience=20):\n",
    "\n",
    "    # Split into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize BiLSTM model, loss, and optimizer\n",
    "    model = BiLSTM(input_size=2, hidden_size=32, output_size=100)\n",
    "    criterion = nn.CrossEntropyLoss()  # For classification\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss_avg:.4f}, Val Loss: {val_loss_avg:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()  # Save best model\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "    # Load best model and save\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    torch.save(model.state_dict(), \"bilstm_model.pth\")\n",
    "    print(\"Model saved to bilstm_model.pth\")\n",
    "    return model\n",
    "\n",
    "# Assuming X, y, and sequences are from your 1000-run data\n",
    "X, y = prepare_training_data(sequences)  # Updated function call\n",
    "print(f\"Training data shape: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "print(\"\\nTraining BiLSTM (Classification)...\")\n",
    "trained_model = train_rnn(X, y, epochs=300, batch_size=32, learning_rate=0.001, patience=20)\n",
    "\n",
    "\n",
    "# Test the model with a few sample sequences\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_X = X[:5] # Already normalized\n",
    "    predictions = trained_model(sample_X).argmax(dim=1) # Get class index\n",
    "    \n",
    "    print(\"\\nSample predictions vs. actual:\")\n",
    "    for i in range(5):\n",
    "        pred_idx = predictions[i].item()\n",
    "        pred_x, pred_y = pred_idx % 10, pred_idx // 10  # Convert index back to (x, y)\n",
    "        print(f\"Sequence {i+1}: {sequences[i]}\")\n",
    "        print(f\"Predicted next: ({pred_x}, {pred_y}), Actual: {sequences[i][5]}\")\n",
    "\n",
    "        #print(f\"Sequence {i+1}: {sequences[i]}\")\n",
    "        #pred = predictions[i].tolist()\n",
    "        #print(f\"Predicted next: ({pred[0]:.2f}, {pred[1]:.2f}), Actual: {sequences[i][5]}\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
